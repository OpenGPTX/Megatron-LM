PYTHONPATH="${PYTHONPATH}:/raid/s3/opengptx/alexw/Megatron-LM/"
CUDA_VISIBLE_DEVICES=1,2
CUDA_DEVICE_MAX_CONNECTIONS=1
# do not remove or the training will hang and nodes will be lost w/o this workaround
# export CUDA_LAUNCH_BLOCKING=1
# force crashing on nccl issues like hanging broadcast
NCCL_ASYNC_ERROR_HANDLING=1
# handle timeouts
NCCL_IB_TIMEOUT=50
UCX_RC_TIMEOUT=4s
NCCL_IB_RETRY_CNT=10
# NCCL and Torch debug
NCCL_DEBUG=INFO
# NCCL_P2P_DISABLE=1